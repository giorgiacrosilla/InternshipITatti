{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    'allenai/Molmo-7B-O-0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'allenai/Molmo-7B-O-0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Specify the input and output folders\n",
    "input_folder = 'aachen_validation_set'  # Folder containing input images\n",
    "output_folder = 'transcriptions_IAM_A-D_molmo'  # Folder to save transcriptions\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Convert image to RGB if it's not already\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    img_array = np.array(image)\n",
    "\n",
    "    # Ensure the image is in the correct shape (height, width, channels)\n",
    "    if img_array.ndim == 2:  # Grayscale image\n",
    "        img_array = np.stack((img_array,) * 3, axis=-1)\n",
    "    elif img_array.shape[2] == 1:  # Single channel\n",
    "        img_array = np.repeat(img_array, 3, axis=-1)\n",
    "\n",
    "    # Return the image as a PIL Image\n",
    "    return Image.fromarray(img_array)\n",
    "\n",
    "# Process each image in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "        # Construct full file paths\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_transcription.txt\")\n",
    "        \n",
    "        # Open the image\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Preprocess the image\n",
    "        image = preprocess_image(image)\n",
    "\n",
    "        # Process the image and text\n",
    "        inputs = processor.process(\n",
    "            images=[image],\n",
    "            text=\"You are an AI assistant specialized in transcribing handwritten text from images. Your task is to focus solely on the handwritten portions of the provided image and transcribe them accurately. Please follow these guidelines: 1. Examine the image carefully and identify all handwritten text. 2. Transcribe ONLY the handwritten text. Ignore any printed or machine-generated text in the image. 3. Maintain the original structure of the handwritten text, including line breaks and paragraphs. 4. Do not attempt to correct spelling or grammar in the handwritten text. Transcribe it exactly as written. Please begin your response directly with the transcribed text. Remember, your goal is to provide an accurate transcription of ONLY the handwritten portions of the text, preserving its original form as much as possible.\"\n",
    "        )\n",
    "\n",
    "        # Move inputs to the correct device and add batch dimension\n",
    "        inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}  # Unsqueeze to add batch dimension\n",
    "\n",
    "        # Generate output; maximum 200 new tokens; stop generation when <|endoftext|> is generated\n",
    "        output = model.generate_from_batch(\n",
    "            inputs,\n",
    "            GenerationConfig(max_new_tokens=200, stop_strings=\"<|endoftext|>\"),\n",
    "            tokenizer=processor.tokenizer\n",
    "        )\n",
    "\n",
    "        # Only get generated tokens; decode them to text\n",
    "        generated_tokens = output[0, inputs['input_ids'].size(1):]\n",
    "        generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Save the transcription to a file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(generated_text)\n",
    "        \n",
    "        print(f\"Transcription for {filename} saved to {output_path}\")\n",
    "\n",
    "print(\"All images processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
