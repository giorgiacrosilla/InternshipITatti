{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fixed paths and prompt\n",
    "INPUT_FOLDER = \"aachen_validation_set\"\n",
    "OUTPUT_FOLDER = \"transcriptions_IAM_ovis\"\n",
    "TRANSCRIPTION_PROMPT = \"You are an AI assistant specialized in transcribing handwritten text from images. Your task is to focus solely on the handwritten portions of the provided image and transcribe them accurately. Please follow these guidelines: 1. Examine the image carefully and identify all handwritten text. 2. Transcribe ONLY the handwritten text. Ignore any printed or machine-generated text in the image. 3. Maintain the original structure of the handwritten text, including line breaks and paragraphs. 4. Do not attempt to correct spelling or grammar in the handwritten text. Transcribe it exactly as written. Please begin your response directly with the transcribed text. Remember, your goal is to provide an accurate transcription of ONLY the handwritten portions of the text, preserving its original form as much as possible.\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"AIDC-AI/Ovis1.6-Gemma2-9B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             multimodal_max_length=8192,\n",
    "                                             trust_remote_code=True).cuda()\n",
    "text_tokenizer = model.get_text_tokenizer()\n",
    "visual_tokenizer = model.get_visual_tokenizer()\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Process each image in the input folder\n",
    "for filename in tqdm(os.listdir(INPUT_FOLDER)):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "        image_path = os.path.join(INPUT_FOLDER, filename)\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        query = f'<image>\\n{TRANSCRIPTION_PROMPT}'\n",
    "\n",
    "        # Format conversation\n",
    "        prompt, input_ids, pixel_values = model.preprocess_inputs(query, [image])\n",
    "        attention_mask = torch.ne(input_ids, text_tokenizer.pad_token_id)\n",
    "        input_ids = input_ids.unsqueeze(0).to(device=model.device)\n",
    "        attention_mask = attention_mask.unsqueeze(0).to(device=model.device)\n",
    "        pixel_values = [pixel_values.to(dtype=visual_tokenizer.dtype, device=visual_tokenizer.device)]\n",
    "\n",
    "        # Generate output\n",
    "        with torch.inference_mode():\n",
    "            gen_kwargs = dict(\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=False,\n",
    "                top_p=None,\n",
    "                top_k=None,\n",
    "                temperature=None,\n",
    "                repetition_penalty=None,\n",
    "                eos_token_id=model.generation_config.eos_token_id,\n",
    "                pad_token_id=text_tokenizer.pad_token_id,\n",
    "                use_cache=True\n",
    "            )\n",
    "            output_ids = model.generate(input_ids, pixel_values=pixel_values, attention_mask=attention_mask, **gen_kwargs)[0]\n",
    "            output = text_tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Save output to a text file\n",
    "        output_filename = os.path.splitext(filename)[0] + '.txt'\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, output_filename)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(output)\n",
    "\n",
    "print(f\"Processing complete. Output files have been saved to {OUTPUT_FOLDER}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
